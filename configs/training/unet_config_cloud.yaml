# UNet Training Configuration - Cloud Production
# ==============================================
#
# Optimized for cloud GPU training:
# - High batch sizes for efficient GPU utilization
# - Full training epochs for production models
# - Mixed precision for faster training
# - Production-ready hyperparameters

# Model configuration
model_name: "document-anonymizer-unet"
version: "v1.0"
base_model: "stabilityai/stable-diffusion-2-inpainting"

# Training parameters - Optimized for GPU training
batch_size: 8                   # High batch size for GPU efficiency
learning_rate: 1.0e-4           # CRITICAL FIX: Increased from 1e-5
num_epochs: 50                  # Full training for production
gradient_accumulation_steps: 4  # Moderate accumulation
mixed_precision: "fp16"         # Enable for faster GPU training
gradient_clipping: 1.0

# Diffusion parameters
num_train_timesteps: 1000
noise_schedule: "scaled_linear"

# Text conditioning - Production settings
text_encoder_lr_scale: 0.1
freeze_text_encoder: false      # Train text encoder for better results

# Optimizer and scheduler - Production settings
optimizer:
  type: "AdamW"
  learning_rate: 1.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]

scheduler:
  type: "cosine_with_restarts"
  warmup_steps: 1000            # Proper warmup for production
  num_cycles: 3                 # Multiple cycles for better convergence
  min_lr_ratio: 0.1

# Storage - Cloud paths
checkpoint_dir: "/workspace/checkpoints/unet"
save_every_n_steps: 2500        # Less frequent saves for production
keep_n_checkpoints: 5           # Keep more checkpoints for production
