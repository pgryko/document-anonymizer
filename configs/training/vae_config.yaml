# VAE Training Configuration with Critical Fixes
# =============================================
# 
# This configuration fixes the most critical bugs in the reference implementations:
# 1. Proper learning rate (5e-4 instead of 5e-6)
# 2. Adequate batch size (16 instead of 2)
# 3. KL divergence loss properly weighted
# 4. Perceptual loss for better text preservation

model:
  name: "document-anonymizer-vae"
  version: "v1.0"
  base_model: "stabilityai/stable-diffusion-2-1-base"

training:
  batch_size: 16              # FIXED: Increased from 2
  learning_rate: 5.0e-4       # CRITICAL FIX: Increased from 5e-6
  num_epochs: 100
  gradient_accumulation_steps: 2
  mixed_precision: "bf16"
  gradient_clipping: 1.0
  
  # CRITICAL FIX: Proper loss configuration including KL divergence
  loss:
    kl_weight: 0.00025        # KL divergence weight (was missing!)
    perceptual_weight: 0.1    # Perceptual loss for text preservation
    recon_loss_type: "mse"
  
  optimizer:
    type: "AdamW"
    weight_decay: 0.01
    betas: [0.9, 0.999]
  
  scheduler:
    type: "cosine_with_restarts"
    warmup_steps: 1000
    num_cycles: 3
    min_lr_ratio: 0.1

dataset:
  train_data_path: "/data/train"
  val_data_path: "/data/val"
  crop_size: 512
  num_workers: 4
  
  # Conservative augmentation for text preservation
  rotation_range: 5.0         # Degrees
  brightness_range: 0.1       # ±10%
  contrast_range: 0.1         # ±10%

storage:
  checkpoint_dir: "/tmp/checkpoints"
  save_every_n_steps: 5000
  keep_n_checkpoints: 3