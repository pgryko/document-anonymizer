# UNet Training Configuration with Critical Fixes
# ==============================================
#
# This configuration fixes critical bugs in the reference implementations:
# 1. Proper learning rate (1e-4 instead of 1e-5)
# 2. Adequate batch size (8 instead of 4)
# 3. Uses Stable Diffusion 2.0 inpainting (correct 9-channel architecture)
# 4. Proper text conditioning with TrOCR

model:
  name: "document-anonymizer-unet"
  version: "v1.0"
  base_model: "stabilityai/stable-diffusion-2-inpainting"  # Correct 9-channel UNet

training:
  batch_size: 8               # FIXED: Increased from 4
  learning_rate: 1.0e-4       # CRITICAL FIX: Increased from 1e-5
  num_epochs: 50
  gradient_accumulation_steps: 4
  mixed_precision: "bf16"
  gradient_clipping: 1.0

  # Diffusion parameters
  num_train_timesteps: 1000
  noise_schedule: "scaled_linear"

  # Text conditioning parameters
  text_encoder_lr_scale: 0.1
  freeze_text_encoder: false

  optimizer:
    type: "AdamW"
    weight_decay: 0.01
    betas: [0.9, 0.999]

  scheduler:
    type: "cosine_with_restarts"
    warmup_steps: 1000
    num_cycles: 3
    min_lr_ratio: 0.1

dataset:
  train_data_path: "/data/train"
  val_data_path: "/data/val"
  crop_size: 512
  num_workers: 4

  # Conservative augmentation for text preservation
  rotation_range: 2.0         # Very conservative for text
  brightness_range: 0.05      # Very conservative
  contrast_range: 0.05        # Very conservative

storage:
  checkpoint_dir: "/tmp/checkpoints"
  save_every_n_steps: 2500
  keep_n_checkpoints: 3
